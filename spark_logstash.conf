input { # The input is a tail on the log4j and stderr files written by spark
  file {
    type => "file"
    path => ["/home/ubuntu/dbrickslogs/myjoblogs/*/driver/log4j-*", "/home/ubuntu/dbrickslogs/myjoblogs/*/executor/app-*/*/stderr"] # path to fuseblob mount point
    codec => multiline {
      pattern => "^%{YEAR}/%{MONTHNUM}/%{MONTHDAY} "
      negate => true
      what => "previous"
      max_lines => 10
      max_bytes => "10 KiB"
    }
    mode => tail
    exclude => "*.gz"  # ignore older files, also change mode to read if the files already exist
    file_completed_action => log
    file_completed_log_path => "/tmp/logstash/logs" #logstash record keeping
    sincedb_path => "/dev/null"
  }
}
filter {  # parseout the data 
   mutate { gsub => [ "message", "\\n", " " ] }
   mutate { gsub => [ "message", "\\r", " " ] }
  grok {
    match => { "message" => "(?m)%{YEAR:year}/%{MONTHNUM:month}/%{MONTHDAY:day} %{HOUR:hour}:%{MINUTE:minute}:%{SECOND:second} %{LOGLEVEL:loglevel} %{DATA:class}: %{GREEDYDATA:logmessage}" }
  }
  mutate { remove_field => [ "host", "tags"] }
  mutate { copy => ["@timestamp", "processtimestamp" ] }
  mutate { add_field => { "eventtimestamp" => "20%{year}-%{month}-%{day} %{hour}:%{minute}:%{second}" } }
}
filter { # ignore if the day field is null. This is due to a few parsing issues above
  if ![day] {
    drop { }
  }
}
output { # write output to kusto using kusto-logstash plugin
    kusto {
            path => "/tmp/kusto1/%{+YYYY-MM-dd-HH-mm-ss}.txt"   # temporary location for kusto plugin 
            ingest_url => "https://ingest-testkusto.westus.kusto.windows.net" # ingest url
            app_id => "my-aad-app-id" # AAD app_id
            app_key => "MYPrettyAppKey=" # app key
            app_tenant => "my-azure-tenant-id" # Azure Tenant
            database => "sparklogdatabase" # kusto database
            table => "mysparklogs" # kusto table
            mapping => "basicmsg"
    }
}
